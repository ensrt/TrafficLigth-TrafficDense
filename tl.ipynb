{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import osmnx as ox\n",
    "import networkx as nx\n",
    "import pygame\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Simülasyon ve trafik grafiği oluşturma\n",
    "center_point = (40.759348, 30.363582)  # Serdivan'a yakın bir koordinat\n",
    "radius = 1250  # 1250 metre\n",
    "graph = ox.graph_from_point(center_point, dist=radius, network_type='all')\n",
    "graph = ox.truncate.largest_component(graph)\n",
    "\n",
    "# Trafik yoğunluğu hesaplayıcı\n",
    "def calculate_traffic_density(agents):\n",
    "    traffic_density = reset_traffic_density()\n",
    "    for agent in agents:\n",
    "        if agent.path and len(agent.path) > 1:\n",
    "            edge = (agent.position, agent.path[1], 0)\n",
    "            if edge in traffic_density:\n",
    "                traffic_density[edge] += 1\n",
    "    return sum(traffic_density.values())\n",
    "\n",
    "# RL Parametreleri\n",
    "alpha = 0.1  # Öğrenme hızı\n",
    "gamma = 0.9  # İndirim faktörü\n",
    "epsilon = 0.1  # Epsilon-greedy araştırma/sömürü oranı\n",
    "episodes = 500_000  # Episode sayısı\n",
    "\n",
    "# Trafik ışıklarının yoğunluğu azaltıp azaltmadığına bağlı olarak ödül fonksiyonu\n",
    "def reward_function(initial_density, final_density):\n",
    "    if final_density < initial_density:\n",
    "        reward = (initial_density - final_density) / initial_density * 100  # Yüzde ödül\n",
    "    else:\n",
    "        reward = -10  # Trafik artarsa ceza\n",
    "    return reward\n",
    "\n",
    "# Q-learning yapısı\n",
    "Q_table = {}  # Q tablosunu başlangıçta boş olarak başlatıyoruz\n",
    "\n",
    "# Trafik yoğunluğu sıfırlama\n",
    "def reset_traffic_density():\n",
    "    return {edge: 0 for edge in graph.edges(keys=True)}\n",
    "\n",
    "# Trafik ışıklarını rastgele koymak için aday kenarları belirliyoruz\n",
    "def get_random_light_locations():\n",
    "    return random.sample(list(graph.nodes), 15)  # 15 rastgele düğüm seçiyoruz\n",
    "\n",
    "# Ajan sınıfı (Araçlar)\n",
    "class Agent:\n",
    "    def __init__(self, graph):\n",
    "        self.graph = graph\n",
    "        self.position = random.choice(list(graph.nodes))\n",
    "        self.path = None\n",
    "        self.speed = random.uniform(1, 2)\n",
    "\n",
    "    def move(self):\n",
    "        if not self.path:\n",
    "            while True:\n",
    "                target = random.choice(list(self.graph.nodes))\n",
    "                try:\n",
    "                    self.path = nx.shortest_path(self.graph, self.position, target)\n",
    "                    break\n",
    "                except nx.NetworkXNoPath:\n",
    "                    continue\n",
    "        \n",
    "        if len(self.path) > 1:\n",
    "            next_position = self.path[1]\n",
    "            self.position = next_position\n",
    "            self.path.pop(0)\n",
    "\n",
    "# Trafik ışıklarını simüle eden RL ajanı\n",
    "def run_rl_simulation(traffic_lights):\n",
    "    agents = [Agent(graph) for _ in range(550)]  # Ajan oluştur\n",
    "    initial_density = calculate_traffic_density(agents)  # Başlangıç yoğunluğu\n",
    "    final_density = calculate_traffic_density(agents)  # Trafik ışıklarıyla oluşan yoğunluk\n",
    "    return initial_density, final_density\n",
    "\n",
    "# Q-learning döngüsü\n",
    "for episode in range(episodes):\n",
    "    # Trafik ışıkları için rastgele bir state (durum) seç\n",
    "    current_state = tuple(get_random_light_locations())\n",
    "\n",
    "    # Q-table girişlerini kontrol et, yoksa başlat\n",
    "    if current_state not in Q_table:\n",
    "        Q_table[current_state] = np.zeros(2)  # İki aksiyon: 0 = ışık koyma, 1 = ışık koyma\n",
    "    \n",
    "    # Epsilon-greedy strateji\n",
    "    if random.uniform(0, 1) < epsilon:\n",
    "        action = random.choice([0, 1])  # Rastgele aksiyon\n",
    "    else:\n",
    "        action = np.argmax(Q_table[current_state])  # En iyi aksiyonu seç\n",
    "    \n",
    "    # Simülasyonu aksiyona göre çalıştır\n",
    "    initial_density, final_density = run_rl_simulation(current_state)\n",
    "\n",
    "    # Ödül hesapla\n",
    "    reward = reward_function(14596, final_density)  # Sabit trafik ışığı olmadan yoğunluk = 14596\n",
    "    \n",
    "    # Q-learning güncelleme\n",
    "    next_state = tuple(get_random_light_locations())  # Bir sonraki rastgele durum\n",
    "    if next_state not in Q_table:\n",
    "        Q_table[next_state] = np.zeros(2)  # İki aksiyon\n",
    "    best_next_action = np.argmax(Q_table[next_state])\n",
    "\n",
    "    Q_table[current_state][action] += alpha * (reward + gamma * Q_table[next_state][best_next_action] - Q_table[current_state][action])\n",
    "\n",
    "    if episode % 1000 == 0:\n",
    "        print(f\"Episode: {episode}, Reward: {reward}\")\n",
    "\n",
    "# En iyi ışık yerleşimlerini bul ve göster\n",
    "best_state = max(Q_table, key=lambda state: np.max(Q_table[state]))\n",
    "print(f\"En iyi trafik ışığı yerleşimleri: {best_state}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
